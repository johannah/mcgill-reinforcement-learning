{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: \n",
    "Bellman Optimality Equations\n",
    "Using a contraction argument, show that there exists a solution to the Bellman optimality equations. That is : show that the Bellman optimality operator is a contraction mapping. (Doina covered the linear case in class ; here you need to go through the same steps but in the nonlinear case).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show that the Bellman Optimality Equations converge to the optimal solution by showing that the\n",
    "the Bellman Optimality Backup is a contraction.  The distance between functions u and v is\n",
    "measured by the max norm, which describes the largest difference between state values: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "{||u-v||}_\\infty = \\mathop{max}_{s \\mathop{\\in} S} \\mathop{|u(s)-v(s)|}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Bellman expectation backup is a contraction defined as $$T^\\pi(v) = R^\\pi + \\mathop{\\gamma}P^{\\pi}v$$\n",
    "It is a $\\gamma$-contraction which makes value functions closer by at least $\\gamma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contraction mapping theorem states that for any metric space $V$ that is complete under an operator $T(v)$ where T is a $\\gamma$-contraction, $T$ converges to a unique fixed point at a linear convergence rate of $\\gamma$. Therefore, the Bellman expectation operator $T^{\\pi}$ has a unique fixed point which contains $v_{\\pi}$. Iterative policy evaluation converges on $v_{\\pi}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman Optimality backup is a contraction defined as\n",
    "\n",
    "\\begin{align}\n",
    "T^*(v) = \\mathop{max}_{a \\in A} R^a + \\gamma P^a v\n",
    "\\end{align}\n",
    "It is a $\\gamma$-contraction which makes the functions closer by at least $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "{||T^{*}(u)-T^{*}(v)||}_\\infty & = {||\\mathop{max}_{a \\in A}(R^a+{\\gamma}P^{a}u) - \\mathop{max}_{a \\in A}(R^a+{\\gamma}P^{a}v)\n",
    "||}_\\infty \\\\\n",
    "&= || \\space{ {||(R^a+{\\gamma}P^{a}u)||}_\\infty - {||(R^a+{\\gamma}P^{a}v)\n",
    "||}_\\infty||}_\\infty \\\\\n",
    "&= || \\space||{{(R^a+{\\gamma}P^{a}u) - (R^a+{\\gamma}P^{a}v)\n",
    "||}_\\infty||}_\\infty \\\\\n",
    "&= || \\space||{{{\\gamma}P^{a}u     - {\\gamma}P^{a}v||}_\\infty||}_\\infty \\\\\n",
    "&\\leq ||{{\\gamma}P^{a}\\space{||u-v||}_\\infty||}_\\infty \\\\\n",
    "&\\leq {\\gamma}\\space{||u-v||}_\\infty\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Problem 3.1\n",
    "Implement and compare empirically the performance of value iteration, policy iteration and modified policy iteration. Modified policy iteration is a simple variant of policy iteration in which the evaluation step is only partial. You can consult the Puterman (1994) textbook for more information. You should first implement the algorithms in a 2-states MDP specified as follows:\n",
    "\n",
    "Transition Probabilities: \n",
    "\n",
    "    P(s_0 | s_0, a_0) = 0.5, \n",
    "    P(s_1 | s_0, a_0) = 0.5, \n",
    "    P(s_0 | s_0, a_1) = 0,\n",
    "    P(s_1 | s_0, a_1) = 1, \n",
    "    P(s_1 | s_0, a_2) = 0, \n",
    "    P(s_1 | s_1, a_2) = 1\n",
    "Rewards: \n",
    "\n",
    "    r(s_0, a_0) = 5, r(s_0, a_1) = 10, r(s_1, a_2) = -1\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mdp\n",
    "import numpy as np\n",
    "num_states = 2\n",
    "num_actions = 3\n",
    "transitions = np.zeros((num_states, num_states, num_actions,), np.float32)\n",
    "rewards = np.zeros((num_states, num_states, num_actions), np.int16)\n",
    "# according to assn given\n",
    "# transitions[s,s1,a]  \n",
    "transitions[0,0,0] = 0.5\n",
    "transitions[0,1,0] = 0.5\n",
    "transitions[0,1,1] = 1.0\n",
    "transitions[1,1,2] = 1.0\n",
    "\n",
    "\n",
    "rewards[0,0,0] = 5\n",
    "rewards[0,1,0] = 5\n",
    "rewards[0,1,1] = 10\n",
    "rewards[1,1,2] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### value iteration\n",
    "\n",
    "def policy_evaluation(policy, ts, rs, gamma):\n",
    "    num_states = ts.shape[0]\n",
    "    num_actions = ts.shape[2]\n",
    "    V = np.zeros(num_states)\n",
    "    theta = .0001\n",
    "    \n",
    "    while True:\n",
    "        e = 0\n",
    "        for s in range(num_states):\n",
    "            v = 0\n",
    "            # look at possible actions from this state\n",
    "            for a in range(num_actions):\n",
    "                for s1 in range(num_states):\n",
    "                    prob = ts[s,s1,a]\n",
    "                    if prob > 0:\n",
    "                        r = rs[s,s1,a]\n",
    "                        v+=prob * (r+gamma*V[s1])\n",
    "            e = max(delta,np.abs(v-V[s]))\n",
    "            V[s] = v\n",
    "            \n",
    "        if e < theta:\n",
    "            break\n",
    "        return V\n",
    "            \n",
    "def policy_improvement(ts, rs, gamma):\n",
    "    num_states = ts.shape[0]\n",
    "    num_actions = ts.shape[2]\n",
    "    p = np.ones([num_states,num_actions])/float(num_actions)\n",
    "    while True:\n",
    "        V = policy_evaluation(p,ts,rs,gamma)\n",
    "        # flag to catch when policy changes\n",
    "        stable = True\n",
    "        for s in range(num_states):\n",
    "            policy_action = np.argmax(p[s])\n",
    "            #av = np.zeros(num_actions)\n",
    "            valid_actions = []\n",
    "            valid_values = []\n",
    "            for a in range(num_actions):\n",
    "                av = 0\n",
    "                is_valid = False\n",
    "                for s1 in range(num_states):\n",
    "                    prob = ts[s,s1,a]\n",
    "                    if prob > 0:\n",
    "                        r = rs[s,s1,a]\n",
    "                        av += prob*(r+gamma*V[s1])\n",
    "                        is_valid = True\n",
    "                if is_valid:\n",
    "                    valid_actions.append(a)\n",
    "                    valid_values.append(av)\n",
    "            best_action = valid_actions[np.argmax(valid_values)]\n",
    "        \n",
    "            if policy_action != best_action:\n",
    "                stable = False\n",
    "            p[s] = np.eye(num_actions)[best_action]\n",
    "            \n",
    "        if stable:\n",
    "            return p, V\n",
    "    \n",
    "    \n",
    "policy = [0,2]\n",
    "policy, V = policy_improvement(transitions,rewards,.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 11.65, 11.65)\n",
      "(1, 2, -1.95, 11.65)\n",
      "(0, 0, 11.65, 0)\n",
      "(1, 2, -1.95, 0)\n",
      "(array([[ 1.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.]]), array([ 11.65,  -1.95]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_action_value(s,ts,rs,gamma):\n",
    "    # find value for actions\n",
    "    valid_actions = []\n",
    "    valid_values = []\n",
    "    for a in range(num_actions):\n",
    "        aval = 0\n",
    "        is_valid = False\n",
    "        for s1 in range(num_states):\n",
    "            prob = ts[s,s1,a]\n",
    "            if prob > 0:\n",
    "                r = rs[s,s1,a]\n",
    "                aval +=prob * (r+gamma*V[s1])\n",
    "                is_valid = True\n",
    "\n",
    "        if is_valid:\n",
    "            valid_actions.append(a)\n",
    "            valid_values.append(aval)\n",
    "\n",
    "    best_action_value = np.max(valid_values)  \n",
    "    best_action = valid_actions[np.argmax(valid_values)]\n",
    "    return best_action_value, best_action\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration(ts,rs,gamma):\n",
    "    num_states = ts.shape[0]\n",
    "    num_actions = ts.shape[2]\n",
    "    V = np.zeros(num_states)\n",
    "    theta = .0001\n",
    "    while True: \n",
    "        e = 0\n",
    "        for s in range(num_states):\n",
    "            best_action_value, best_action = get_action_value(s,ts,rs,gamma)\n",
    "            e = max(e,np.abs(best_action_value-V[s]))\n",
    "            V[s] = best_action_value\n",
    "            print(s,best_action,best_action_value,e)\n",
    "        if e<theta:\n",
    "            break\n",
    "        policy = np.zeros([num_states, num_actions])\n",
    "\n",
    "        \n",
    "    for s in range(num_states):\n",
    "        best_action_value, best_action = get_action_value(s,ts,rs,gamma)\n",
    "        policy[s,best_action] = 1.0\n",
    "    return policy,V\n",
    "            \n",
    "pol,v=value_iteration(transitions,rewards,.95)         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
